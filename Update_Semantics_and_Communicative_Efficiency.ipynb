{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Update Semantics and Communicative Efficiency.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iG41AE_ltz4E"
      },
      "source": [
        "# Update Semantics and Communicative Efficiency\n",
        "\n",
        "This is the notebook accompanying the thesis, *Update Semantics and Informativeness in Natural Language* by Aram Elias, for the 2021 BSc Artificial Intelligence at the University of Amsterdam."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqDO1L1a3Gjz"
      },
      "source": [
        "## Importing Files and Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G90XVys-zlJ-",
        "outputId": "d846d0ab-8b85-4e50-d74e-740ba9016f11"
      },
      "source": [
        "# Google Colab drive mounting\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "#file_path = \"/content/gdrive/My Drive/Colab Notebooks/\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9d162lNBxKX2"
      },
      "source": [
        "import os.path\n",
        "import pickle\n",
        "from functools import reduce, lru_cache\n",
        "from itertools import combinations\n",
        "from pprint import pprint\n",
        "from types import FunctionType\n",
        "from itertools import product\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import argparse\n",
        "from glob import glob\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "from __future__ import division\n",
        "from fractions import Fraction\n",
        "import csv"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJUv6KlJtDmx"
      },
      "source": [
        "# Boolean Expressivist *Language of Thought*\n",
        "\n",
        "Based on the paper *Updates and Boolean Universals* by F. Carcassi and G. Sbardolini (2021)\n",
        "\n",
        "Original code from: https://github.com/thelogicalgrammar/booleanExpressivistLOT\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgLD2Bzeygy3"
      },
      "source": [
        "## Tables for Reference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEkNHaATkgMT"
      },
      "source": [
        "Based on the tables in Carcassi and Sbardolini (2021).\n",
        "\n",
        "**Truth Tables and Updates**\n",
        "\n",
        "| Formula | Truth Table | Update &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
        "| --- | --- | --- |\n",
        "| `T` | 1111 | $c$ |\n",
        "| `F` | 0000 | $c[^-c]$ |\n",
        "| $p$ | 1100 | $c[^+p]$ |\n",
        "| $q$ | 1010 | $c[^+q]$ |\n",
        "| `not` $p$ | 0011 | $c[^-p]$ |\n",
        "| `not` $q$ | 0101 | $c[^-q]$ |\n",
        "| $p$ `and` $q$ | 1000 | $(c[^+p])[^+q]$ |\n",
        "| $p$ `or` $q$ | 1110 | $c[^+p, ^+q]$      |\n",
        "| $p$ `nor` $q$ | 0001 | $(c[^-p])[^-q]$ |\n",
        "| $p$ `nand` $q$ | 0111 | $c[^-(c[^+p])[^+q]]$ |\n",
        "| $p$ `XOR` $q$ | 0110 | $(c[^-(c[^+p])[^+q])])[^-(c[^-p])[^-q]]$ |\n",
        "| $p$ `bc` $q$ | 1001 | $c[^-(c[^-(c[^+p])[^+q])])[^-(c[^-p])[^-q]]]$ |\n",
        "| $p$ `c` $q$ | 1011 | $c[^-(c[^+p])[^-q]]$ |\n",
        "| $p$ `ic` $q$ | 1101 | $c[^-(c[^-p])[^+q]]$ |\n",
        "| $p$ `nc` $q$ | 0100 | $(c[^+p])[^-q]$ |\n",
        "| $p$ `nic` $q$ | 0010 | $(c[^-p])[^+q]$ |\n",
        "\n",
        "**Complexity of Connectives**\n",
        "\n",
        "| Connective | Local | Homogeneous | Positive | Complexity |\n",
        "| --- | --- | --- | --- | --- |\n",
        "| `not` ($\\neg$) | $\\checkmark$ | $\\checkmark$ | | 2 | \n",
        "| `and` ($\\wedge$) | $\\checkmark$ | $\\checkmark$ | $\\checkmark$ | 1 |\n",
        "| `or` ($\\vee$) | $\\checkmark$ | $\\checkmark$ | $\\checkmark$ | 1 |\n",
        "| `nor` ($\\downarrow$) | $\\checkmark$ | $\\checkmark$ | | 2 |\n",
        "| `nand` ($\\uparrow$) | | | | 4 |\n",
        "| `XOR` ($\\nleftrightarrow$) | | | | 4 |\n",
        "| `bc` ($\\leftrightarrow$) | | | | 4 |\n",
        "| `c` ($\\rightarrow$) | | | | 4 |\n",
        "| `ic` ($\\leftarrow$) | | | | 4 |\n",
        "| `nc` ($\\nrightarrow$) | $\\checkmark$ | | | 3 |\n",
        "| `nic` ($\\nleftarrow$) | $\\checkmark$ | | | 3 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZm_Pyi3uu6P"
      },
      "source": [
        "## From analysis.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r99-DPjjuybv"
      },
      "source": [
        "attested_languages = (\n",
        "    frozenset(['nor', 'and', 'or', 'not']),\n",
        "    frozenset(['and', 'or', 'not']),\n",
        "    frozenset(['and', 'not']),\n",
        "    frozenset(['or', 'not']),\n",
        ")\n",
        "\n",
        "def plot_languages(dict_usage_complexities, dict_cognitive_complexity):\n",
        "    \"\"\"\n",
        "    Plot the languages stored in the dictionaries\n",
        "    \"\"\"\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(8.27,4))\n",
        "    for name in dict_usage_complexities.keys():\n",
        "\n",
        "        # if not any([i in ['nc', 'nic', 'bc', 'XOR', 'c', 'ic'] for i in name]) and 'not' in name:\n",
        "        if 'not' in name:\n",
        "        # if True:\n",
        "\n",
        "            usage_complexity = dict_usage_complexities[name]\n",
        "            cognitive_complexity = dict_cognitive_complexity[name]\n",
        "\n",
        "            if name in attested_languages:\n",
        "                color = 'red'\n",
        "                zorder = 10\n",
        "                if name == frozenset(['or', 'not']):\n",
        "                    yshift = 0.4\n",
        "                else:\n",
        "                    yshift = 0\n",
        "                ax.text(\n",
        "                    usage_complexity + 0.02,\n",
        "                    cognitive_complexity + 0.3 + yshift,\n",
        "                    s=','.join(name),\n",
        "                    fontsize='x-small'\n",
        "                )\n",
        "            else:\n",
        "                color='black'\n",
        "                zorder = 1\n",
        "\n",
        "            ax.scatter(usage_complexity,cognitive_complexity,color=color)\n",
        "\n",
        "    ax.set_xlabel('Usage complexity')\n",
        "    ax.set_ylabel('Conceptual complexity')\n",
        "    ax.set_xlim(1.05,2.8)\n",
        "\n",
        "    plt.savefig('figure.png', dpi=300, transparent=True)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0TvFoUPxjJI"
      },
      "source": [
        "def get_minimal_languages(folder):\n",
        "    languages = {}\n",
        "    for path in glob(folder+'/*.pickle'):\n",
        "        with open(path, 'rb') as openfile:\n",
        "            booleans, formulas = pickle.load(openfile)\n",
        "            # dictionary = {b:f for b,f in zip(booleans,formulas)}\n",
        "            dictionary = np.column_stack((booleans,formulas))\n",
        "            name = frozenset(\n",
        "                os.path.splitext(os.path.basename(path))[0]\n",
        "                .split('_')\n",
        "            )\n",
        "            languages[name] = dictionary\n",
        "    return languages"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJudH1WAuYfA"
      },
      "source": [
        "## From language_parser.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbhS7-ShdXfG"
      },
      "source": [
        "### Indentation Levels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0VeGIT2ueqg"
      },
      "source": [
        "# Original function\n",
        "def get_indentation_levels_old(formula, exclude_brackets):\n",
        "    if type(formula)==str:\n",
        "        formula_np = np.array(list(formula)) \n",
        "    position_closed = formula_np==')'\n",
        "    formula_open = formula_np=='('\n",
        "    formula_close = np.zeros(len(position_closed))\n",
        "    formula_close[1:] = (np.where(position_closed,-1,0))[:-1]\n",
        "    formula_level = (formula_close+formula_open).cumsum()\n",
        "    if exclude_brackets:\n",
        "        brackets_position = formula_open | position_closed\n",
        "        formula_without_brackets = formula_np[~brackets_position]\n",
        "        formula_level_without_brackets = formula_level[~brackets_position]\n",
        "        return formula_without_brackets, formula_level_without_brackets\n",
        "    else:\n",
        "        return formula_np, formula_level"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAFqmHOcMiyd"
      },
      "source": [
        "# Optimized (iterates through formula only once)\n",
        "def get_indentation_levels(formula, exclude_brackets):\n",
        "    if type(formula) == str:\n",
        "        formula = np.array(list(formula))\n",
        "    formula_out = []\n",
        "    levels = []\n",
        "    level = 0\n",
        "    for c in formula:\n",
        "        if c == '(':\n",
        "            level += 1\n",
        "            if not exclude_brackets:\n",
        "                formula_out.append(c)\n",
        "                levels.append(level)\n",
        "        elif c == ')':\n",
        "            if not exclude_brackets:\n",
        "                formula_out.append(c)\n",
        "                levels.append(level)\n",
        "            level -= 1\n",
        "        else:\n",
        "            formula_out.append(c)\n",
        "            levels.append(level)\n",
        "    return np.array(formula_out), np.array(levels)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2dYETkSddbl"
      },
      "source": [
        "### String Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vG6-1IwWunDP"
      },
      "source": [
        "def np_to_string(f):\n",
        "    return f.astype('|S1').tobytes().decode('utf-8')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0KOcj3UuolI"
      },
      "source": [
        "def separate(f,levs):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    f: array\n",
        "        np array of characters representing the formula\n",
        "    levs: array\n",
        "        Int array of the nesting level of f\n",
        "        Where argument levels is shifted down so it starts from 0\n",
        "    Returns\n",
        "    -------\n",
        "    list of lists\n",
        "        Each list contains a tuple (argument f array, argument levels)\n",
        "    \"\"\"\n",
        "    assert len(f)==len(levs), 'formula and scope have different lens'\n",
        "    # one comma for every argument in main scope.\n",
        "    # possibly 0, e.g. for negation\n",
        "    mask_commas_in_main = (f==',')&(levs==0)\n",
        "    indices_commas_in_main = np.argwhere(mask_commas_in_main).flatten()\n",
        "    # f without commas in main scope\n",
        "    f_without_commas = f[~mask_commas_in_main]\n",
        "    f_separated = np.split(f_without_commas, indices_commas_in_main)\n",
        "    levs_without_commas = levs[~mask_commas_in_main]\n",
        "    levs_separated = np.split(levs_without_commas, indices_commas_in_main)\n",
        "    return zip(f_separated, levs_separated)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMMffbW9dg62"
      },
      "source": [
        "### Formula Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTEthGU9uqgf"
      },
      "source": [
        "def eval_formula_recursive(f_wo_brackets, level_wo_brackets, m_dict):\n",
        "    # if it's a basic formula, return its meaning\n",
        "    if np.all(level_wo_brackets==0):\n",
        "        return m_dict[np_to_string(f_wo_brackets)]\n",
        "\n",
        "    # if it's not a basic formula, run this function \n",
        "    # on its components and then combine the meanings\n",
        "    operator = np_to_string(f_wo_brackets[level_wo_brackets==0])\n",
        "    operator_m = m_dict[operator]\n",
        "\n",
        "    # split the arguments\n",
        "    inner_indices = level_wo_brackets > 0\n",
        "    arguments_tuples = separate(\n",
        "        f_wo_brackets[inner_indices],\n",
        "        level_wo_brackets[inner_indices]-1) \n",
        "\n",
        "    # run the function on the arguments\n",
        "    args_meanings = [\n",
        "        eval_formula_recursive(f,lev,m_dict)\n",
        "        for f, lev in arguments_tuples]\n",
        "\n",
        "    return operator_m(*args_meanings)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIn0f6B8useH"
      },
      "source": [
        "def evaluate_formula(formula,m_dict):\n",
        "    \"\"\"\n",
        "    The assumption is that the formulas have shape\n",
        "        OP(formula,formula)\n",
        "    NOTE: no spaces\n",
        "    If a formula contains brackets, it should be analysed further\n",
        "    Otherwise it's a basic expression\n",
        "    \"\"\"\n",
        "    # find the nesting level of every character in the formula\n",
        "    f_wo_brackets, level_wo_brackets = get_indentation_levels(formula,True)\n",
        "    return eval_formula_recursive(\n",
        "        f_wo_brackets,\n",
        "        level_wo_brackets,\n",
        "        m_dict\n",
        "    )"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJ7dOomRtk0i"
      },
      "source": [
        "## From utilities.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dpy_kAL_tymx"
      },
      "source": [
        "def bit_not(n, numbits=4):\n",
        "    return (1<<numbits)-1-n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aatTPBaPt1iI"
      },
      "source": [
        "def combine(f_1, f_2, first_top_unsaturated):\n",
        "    \"\"\"\n",
        "    Substitute the first argument on top level in f_1 for f_2\n",
        "    \"\"\"\n",
        "    # Old implementation which adds simply\n",
        "    # to the first occurrence of _\n",
        "    # return f_1.replace('_',f_2,1)\n",
        "    return np.concatenate((\n",
        "        # everything before the _\n",
        "        f_1[:first_top_unsaturated],\n",
        "        # the argument itself\n",
        "        f_2,\n",
        "        # everything after the _\n",
        "        f_1[first_top_unsaturated+1:]\n",
        "    ))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qE6tMnEht6O4"
      },
      "source": [
        "def find_n_args(f):\n",
        "    \"\"\"\n",
        "    Finds the number of arguments of a function\n",
        "    \"\"\"\n",
        "    return f.__code__.co_argcount"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIcXlY7fdmPF"
      },
      "source": [
        "### Checking Symmetry"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffl7fxj7t794"
      },
      "source": [
        "def check_symmetry(new_node, new_nodes, pivot):\n",
        "    \"\"\"\n",
        "    check that new_node is not symmetric to any of new_nodes\n",
        "    First flips the formula around the pivot, \n",
        "    Then checks that the flipped version is not identical to any of\n",
        "    the nodes in new_nodes\n",
        "    Parameters\n",
        "    ----------\n",
        "    new_node: string\n",
        "        The node that is proposed to add to the list of new nodes\n",
        "    new_nodes: list of strings\n",
        "        The nodes that have already been added in that generation\n",
        "    levels_top_unsat: boolean array \n",
        "        For each char in new_node, whether the char is the same level\n",
        "    \"\"\"\n",
        "    assert new_node[pivot]==',', 'Pivot isnt a comma'\n",
        "    # the first argument returned is simply new_node\n",
        "    f, levels = get_indentation_levels(new_node, False)\n",
        "    # get the last open bracket before the pivot\n",
        "    # at the same indentation level as the pivot\n",
        "    before_p = np.nonzero(\n",
        "        (f[:pivot]=='(') & (levels[:pivot]==levels[pivot])\n",
        "    )[0][-1]\n",
        "    # get first closed bracket after the pivot\n",
        "    # at the same indentation level of the pivot\n",
        "    after_p = pivot + np.nonzero(\n",
        "        (f[pivot:]==')') & (levels[pivot:]==levels[pivot])\n",
        "    )[0][0]\n",
        "\n",
        "    flipped_node = np.concatenate((\n",
        "        # include the opening bracket\n",
        "        f[:before_p+1],\n",
        "        # from the pivot to before the closing bracket (second arg)\n",
        "        f[pivot+1:after_p],\n",
        "        # comma corresponding to the pivot\n",
        "        np.array([',']),\n",
        "        # from the after the opening bracket to pivot\n",
        "        f[before_p+1:pivot],\n",
        "        # everything after and including the closing bracket\n",
        "        f[after_p:]\n",
        "    ))\n",
        "    return np_to_string(flipped_node) in new_nodes"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmbVGo8P3fUC"
      },
      "source": [
        "# isolated version of the flip used in check_symmetry\n",
        "def flip_formula(formula, pivot):\n",
        "    assert formula[pivot]==',', 'Pivot isnt a comma'\n",
        "    f, levels = get_indentation_levels(formula, False)\n",
        "    # get the last open bracket before the pivot\n",
        "    # at the same indentation level as the pivot\n",
        "    before_p = np.nonzero(\n",
        "        (f[:pivot]=='(') & (levels[:pivot]==levels[pivot])\n",
        "    )[0][-1]\n",
        "    # get first closed bracket after the pivot\n",
        "    # at the same indentation level of the pivot\n",
        "    after_p = pivot + np.nonzero(\n",
        "        (f[pivot:]==')') & (levels[pivot:]==levels[pivot])\n",
        "    )[0][0]\n",
        "\n",
        "    return np_to_string(np.concatenate((\n",
        "        f[:before_p+1], f[pivot+1:after_p],\n",
        "        np.array([',']), f[before_p+1:pivot],\n",
        "        f[after_p:]\n",
        "    )))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcDdt8Al4cim"
      },
      "source": [
        "# switch p and q to see if this formula already exists\n",
        "def switch_formula(formula):\n",
        "    formula = list(formula)\n",
        "    for i in range(len(formula)):\n",
        "        c = formula[i]\n",
        "        if c == 'p':\n",
        "            formula[i] = 'q'\n",
        "        elif c == 'q':\n",
        "            formula[i] = 'p'\n",
        "    return ''.join(formula)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpLHPjmGdoTF"
      },
      "source": [
        "### Find Minimal Formulas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-id4VtBKt-TY"
      },
      "source": [
        "def find_shortest_formulas(m_dict, nesting_limit=7, verbose=True):\n",
        "    \"\"\"\n",
        "    Since formulas are added to saturated in order of complexity,\n",
        "    whenever I encounter a meaning that's already in \n",
        "    saturated, I don't need to add it to saturated\n",
        "    Because the new meaning is going to be at least\n",
        "    as complex as the one that's alread in saturated\n",
        "    Otherwise, add to saturated\n",
        "    NOTE: this relies on the fact that the shortest\n",
        "    formula for a meaning is a combination of shortest formulas \n",
        "    NOTE: lexicon should contain first p and q, and then\n",
        "    the other terms in order of complexity\n",
        "    NOTE: the nesting_limit e.g. =7 means that it will\n",
        "    only calculate formulas up to 7 and make 'fake'\n",
        "    formulas of length 8 for everything else\n",
        "    \"\"\"\n",
        "\n",
        "    saturated, saturated_m, unsaturated, unsaturated_m = [], [], [], []\n",
        "\n",
        "    for k,v in m_dict.items():\n",
        "        if type(v)==FunctionType:\n",
        "            n_args = find_n_args(v)\n",
        "            unsaturated_m.append(v)\n",
        "            unsaturated.append(k + '(' + ','.join(['_']*n_args) + ')')\n",
        "        else:\n",
        "            saturated.append(k)\n",
        "            saturated_m.append(v)\n",
        "\n",
        "    # store the formulas and meanings for \n",
        "    # each of the unique expressions\n",
        "    # store unique formulas the you already know the meaning of here\n",
        "    unique_formulas = [*saturated]\n",
        "    # store the relative meanings here \n",
        "    # NOTE: in the same order\n",
        "    unique_meanings = [*saturated_m]\n",
        "    # store the lexicon (which contains the expressions\n",
        "    # with parentheses like 'not(_)')\n",
        "    lexicon = [*saturated, *unsaturated]\n",
        "    lexicon = [np.array(list(a)) for a in lexicon]\n",
        "    # start with just the initial unsaturated in it\n",
        "    # unsaturated terminal nodes\n",
        "    terminal_nodes = [*unsaturated]\n",
        "    while len(unique_meanings) < 16:\n",
        "        new_nodes = []\n",
        "        # loop through the terminal nodes\n",
        "        for current_node in terminal_nodes:\n",
        "            # loop through the lexicon\n",
        "            # and try to add each of the saturated\n",
        "            for n in lexicon:\n",
        "                # replace the first occurrence of _ on the top\n",
        "                # nesting level of current_node with lexicon entry n\n",
        "                f, levels = get_indentation_levels(\n",
        "                    current_node, False)\n",
        "                first_top_unsaturated = np.argmin(\n",
        "                    np.where(f=='_',levels,np.inf))\n",
        "                new_node = np_to_string(combine(f, n, first_top_unsaturated))\n",
        "\n",
        "                # store the levels of the substitution to use it later\n",
        "                # to find the pivot\n",
        "                levels_top_unsat = (\n",
        "                    (levels==levels[first_top_unsaturated])&(f=='_'))\n",
        "\n",
        "                # when encountering a saturated node, \n",
        "                # check if it is in unique already\n",
        "                # NOTE: not equivalent to nested ifs\n",
        "                if '_' not in new_node:\n",
        "                    new_meaning = evaluate_formula(new_node, m_dict)\n",
        "                    if new_meaning not in unique_meanings:  \n",
        "                        unique_formulas.append(new_node)\n",
        "                        unique_meanings.append(new_meaning)\n",
        "                        if verbose:\n",
        "                            print((new_node, new_meaning))\n",
        "                else:\n",
        "                    # assert levels_top_unsat.sum() < 3, \"Strange!\"\n",
        "                    # NOTE: if the character before the '_' is an open\n",
        "                    # bracket, add directly without checking for symmetry\n",
        "                    # (python's 'or' short-circuits, so if the first\n",
        "                    # condition is true the second isn't evaluated)\n",
        "                    # The condition isn't saved if it's the second arg\n",
        "                    # AND it's symmetric to a previous expression\n",
        "                    if (\n",
        "                            new_node[first_top_unsaturated-1]=='(' or \n",
        "                            # TODO: only if the operation is symmetrc\n",
        "                            not check_symmetry(\n",
        "                                new_node,\n",
        "                                new_nodes,\n",
        "                                first_top_unsaturated-1\n",
        "                            )):\n",
        "                        new_nodes.append(new_node)\n",
        "                # breakpoint()\n",
        "        # new_nodes, new_m = reduce_symmetric_operators(new_nodes, new_m)\n",
        "        terminal_nodes = new_nodes\n",
        "        if verbose:\n",
        "            print(len(terminal_nodes),'')\n",
        "            print('len of last: ', terminal_nodes[-1].count('('))\n",
        "        # print(unique_formulas, '\\n')\n",
        "        # print(terminal_nodes[0])\n",
        "        # this if means that it has calculated all the formulas up to and including\n",
        "        # the formulas of length nesting_limit\n",
        "        if terminal_nodes[-1].count('(') == nesting_limit:\n",
        "            n_remaining = 16 - len(unique_formulas)\n",
        "            unique_meanings += [-0000]*n_remaining\n",
        "            unique_formulas += ['('*(nesting_limit+1)]*n_remaining\n",
        "            if verbose: print(unique_formulas, '\\n')\n",
        "\n",
        "    return [f'{x:04b}' for x in unique_meanings], unique_formulas"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_AExGMLf4Hc"
      },
      "source": [
        "def find_simplest_formulas(m_dict, nesting_limit=7, verbose=True):\n",
        "    \"\"\"\n",
        "    Shorter formulas are not necessarily simpler in terms of cognitive\n",
        "    complexity. This means we should be able to replace formulas encountered\n",
        "    earlier with more cognitively simple ones.\n",
        "    NOTE: this relies on the fact that the shortest\n",
        "    formula for a meaning is a combination of shortest formulas \n",
        "    NOTE: lexicon should contain first p and q, and then\n",
        "    the other terms in order of complexity\n",
        "    NOTE: the nesting_limit e.g. =7 means that it will\n",
        "    only calculate formulas up to 7 and make 'fake'\n",
        "    formulas of length 8 for everything else\n",
        "    \"\"\"\n",
        "    \n",
        "    saturated, saturated_m, unsaturated, unsaturated_m = [], [], [], []\n",
        "\n",
        "    for k,v in m_dict.items():\n",
        "        if type(v)==FunctionType:\n",
        "            n_args = find_n_args(v)\n",
        "            unsaturated_m.append(v)\n",
        "            unsaturated.append(k + '(' + ','.join(['_']*n_args) + ')')\n",
        "        else:\n",
        "            saturated.append(k)\n",
        "            saturated_m.append(v)\n",
        "\n",
        "    # store the lexicon (which contains the expressions\n",
        "    # with parentheses like 'not(_)')\n",
        "    lexicon = [*saturated, *unsaturated]\n",
        "    lexicon = [np.array(list(a)) for a in lexicon]\n",
        "    # start with just the initial unsaturated in it\n",
        "    # unsaturated terminal nodes\n",
        "    terminal_nodes = [*unsaturated]\n",
        "\n",
        "    simplest_dict = dict(zip(saturated_m, saturated))\n",
        "\n",
        "    while terminal_nodes != [] and terminal_nodes[-1].count('(') != nesting_limit:\n",
        "        new_nodes = []\n",
        "        # loop through the terminal nodes\n",
        "        for current_node in terminal_nodes:\n",
        "            # loop through the lexicon\n",
        "            # and try to add each of the saturated\n",
        "            for n in lexicon:\n",
        "                # replace the first occurrence of _ on the top\n",
        "                # nesting level of current_node with lexicon entry n\n",
        "                f, levels = get_indentation_levels(\n",
        "                    current_node, False)\n",
        "                first_top_unsaturated = np.argmin(\n",
        "                    np.where(f=='_',levels,np.inf))\n",
        "                new_node = np_to_string(combine(f, n, first_top_unsaturated))\n",
        "\n",
        "                # store the levels of the substitution to use it later\n",
        "                # to find the pivot\n",
        "                levels_top_unsat = (\n",
        "                    (levels==levels[first_top_unsaturated])&(f=='_'))\n",
        "\n",
        "                # when encountering a saturated node, \n",
        "                # check if A SIMPLER ONE with the same meaning is already there\n",
        "                # NOTE: not equivalent to nested ifs\n",
        "                if '_' not in new_node:\n",
        "                    new_meaning = evaluate_formula(new_node, m_dict)\n",
        "                    if new_meaning not in simplest_dict.keys():\n",
        "                        simplest_dict[new_meaning] = new_node\n",
        "                    else:\n",
        "                        old_complexity = calculate_formula_complexity(\n",
        "                            simplest_dict[new_meaning]\n",
        "                        )\n",
        "                        new_complexity = calculate_formula_complexity(new_node)\n",
        "                        if new_complexity < old_complexity:\n",
        "                            simplest_dict[new_meaning] = new_node\n",
        "\n",
        "                else:\n",
        "                    # NOTE: if the character before the '_' is an open\n",
        "                    # bracket, add directly without checking for symmetry\n",
        "                    if new_node[first_top_unsaturated-1] == '(':\n",
        "                        check_flipped = False\n",
        "                    # check if a flipped or switched version is in new_nodes\n",
        "                    else:\n",
        "                        # TODO: only if the operation is symmetric\n",
        "                        check_flipped = flip_formula(new_node,\n",
        "                            first_top_unsaturated-1) in new_nodes\n",
        "                    check_switched = switch_formula(new_node) in new_nodes\n",
        "                    if not check_flipped and not check_switched:\n",
        "                        new_nodes.append(new_node)\n",
        "\n",
        "            terminal_nodes = new_nodes\n",
        "            if verbose:\n",
        "                print('nodes: ', len(terminal_nodes))\n",
        "                print('len of shortest: ',\n",
        "                      min([n.count('(') for n in terminal_nodes]))\n",
        "                print('len of last: ', terminal_nodes[-1].count('('))\n",
        "                print(simplest_dict)\n",
        "\n",
        "    truth = list(simplest_dict.keys())\n",
        "    return [f'{x:04b}' for x in truth], [simplest_dict[key] for key in truth]    "
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPCleG8df4n1"
      },
      "source": [
        "### Functionally Complete Languages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxT3K9wbuCkp"
      },
      "source": [
        "f_complete = [\n",
        "    # one element\n",
        "    ('nor',),\n",
        "    ('nand',),\n",
        "    # two elements\n",
        "    ('or', 'not'),\n",
        "    ('and', 'not'),\n",
        "    ('c', 'not'),\n",
        "    ('ic', 'not'),\n",
        "    #('c', 'F'), # optional\n",
        "    #('ic', 'F'), # optional\n",
        "    ('c', 'XOR'),\n",
        "    ('ic', 'XOR'),\n",
        "    ('c', 'nc'),\n",
        "    ('c', 'nic'),\n",
        "    ('ic', 'nc'),\n",
        "    ('ic', 'nic'),\n",
        "    ('nc', 'not'),\n",
        "    ('nic', 'not'),\n",
        "    #('nc', 'T'), # optional\n",
        "    #('nic', 'T'), # optional\n",
        "    ('nc', 'bc'),\n",
        "    ('nic', 'bc'),\n",
        "    # three elements\n",
        "    #('or', 'bc', 'F'), # optional\n",
        "    ('or', 'bc', 'XOR'),\n",
        "    #('or', 'XOR', 'T'), # optional\n",
        "    #('and', 'bc', 'F'), # optional\n",
        "    ('and', 'bc', 'XOR'),\n",
        "    #('and', 'XOR', 'T') # optional\n",
        "]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaQVee_YibH7"
      },
      "source": [
        "def calculate_functionally_complete():\n",
        "    # calculate all combinations of signals\n",
        "    # of up to 5 elements\n",
        "    # which are supersets of at least one of the sets in f_complete\n",
        "    f_all = []\n",
        "    # list of the binary operators\n",
        "    names = ['not', 'or', 'and', 'nand', 'nor', \n",
        "            'c', 'ic', 'nc', 'nic', 'bc', 'XOR']\n",
        "    for i in range(1,6):\n",
        "        for signals_combination in combinations(names, r=i):\n",
        "            # print([sig in sig_f_complete for sig_f_complete in f_complete])\n",
        "            if any([\n",
        "                all([sig in signals_combination for sig in sig_complete_tuple])\n",
        "                for sig_complete_tuple in f_complete]):\n",
        "                f_all.append(signals_combination)\n",
        "\n",
        "    return f_all"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYasG38DuE9R"
      },
      "source": [
        "def calculate_for_all_functionally_complete(m_dict, verbose=True):\n",
        "    f_all = calculate_functionally_complete()\n",
        "\n",
        "    if verbose: pprint(f_all)\n",
        "    dictionary_complete = {}\n",
        "    for complete_set in f_all:\n",
        "        m_dict_restricted = {k:m_dict[k] for k in complete_set}\n",
        "        m_dict_restricted.update({\n",
        "            'p': m_dict['p'],\n",
        "            'q': m_dict['q']\n",
        "        })\n",
        "        shortest = find_shortest_formulas(m_dict_restricted, verbose=verbose)\n",
        "        if verbose:\n",
        "            print(complete_set)\n",
        "            print(np.column_stack(shortest),'\\n')\n",
        "        dictionary_complete[complete_set]=np.column_stack(shortest)\n",
        "    return dictionary_complete"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TthP-gSYdtdl"
      },
      "source": [
        "### Expected Complexity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mT_fdiTfuG2v"
      },
      "source": [
        "def calculate_expected_complexity(dict_complete):\n",
        "    \"\"\"\n",
        "    Calculates expected complexity for each lang,\n",
        "    where complexity is calculated as the number of operations \n",
        "    in the formula, found as the number of '('.\n",
        "    Parameters\n",
        "    ----------\n",
        "    dict_complete: dict\n",
        "        Dictionary where each key is a tuple (or ordered dict) of primitives\n",
        "        and each value is \n",
        "    \"\"\"\n",
        "    expected_complexity_dict = {}\n",
        "    for tuple_primitives, shortest_defs in dict_complete.items():\n",
        "        expected_complexity_dict[tuple_primitives] = np.sum([\n",
        "            s.count('(') for s in shortest_defs[:,1]\n",
        "        ])/len(shortest_defs)\n",
        "    return expected_complexity_dict"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Tj6MbM0d1pR"
      },
      "source": [
        "# Measure of cognitive complexity, used by multiple functions now\n",
        "homogeneity_cost = 1\n",
        "negation_cost = 1\n",
        "globality_cost = 1\n",
        "binary_cost = 1\n",
        "unary_cost = 1\n",
        "\n",
        "complexities = {\n",
        "    'not': unary_cost + negation_cost,\n",
        "    'and': binary_cost,\n",
        "    'or': binary_cost,\n",
        "    'nor': binary_cost + negation_cost,\n",
        "    'nand': binary_cost + globality_cost + negation_cost + homogeneity_cost,\n",
        "    'XOR': binary_cost + globality_cost + negation_cost + homogeneity_cost,\n",
        "    'bc': binary_cost + globality_cost + negation_cost + homogeneity_cost,\n",
        "    'c': binary_cost + globality_cost + negation_cost + homogeneity_cost,\n",
        "    'ic': binary_cost + globality_cost + negation_cost + homogeneity_cost,\n",
        "    'nc': binary_cost + negation_cost + homogeneity_cost,\n",
        "    'nic': binary_cost + negation_cost + homogeneity_cost\n",
        "}\n",
        "# complexities = {\n",
        "#     'not': 1,\n",
        "#     'and': 1,\n",
        "#     'or': 1,\n",
        "#     'nor': 1,\n",
        "#     'nand': 2,\n",
        "#     'XOR': 3,\n",
        "#     'bc': 3,\n",
        "#     'c': 2,\n",
        "#     'ic': 2,\n",
        "#     'nc': 1,\n",
        "#     'nic': 1\n",
        "# }"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cv6VwhDEuI2X"
      },
      "source": [
        "# Applies to a whole language\n",
        "def calculate_language_complexity(lang_dict):\n",
        "    \"\"\"\n",
        "    # negation_cost = 1\n",
        "    # complexities = {\n",
        "    #     'not': 1 + negation_cost,\n",
        "    #     'and': 1,\n",
        "    #     'or': 1,\n",
        "    #     'nor': 1 + 2*negation_cost,\n",
        "    #     'nand': 2 + 2*negation_cost,\n",
        "    #     'XOR': 3 + negation_cost,\n",
        "    #     'bc': 3,\n",
        "    #     'c': 2 + negation_cost,\n",
        "    #     'ic': 2 + negation_cost,\n",
        "    #     'nc': 1 + negation_cost,\n",
        "    #     'nic': 1 + negation_cost\n",
        "    # }\n",
        "    \"\"\"\n",
        "\n",
        "    complexity = {\n",
        "        key: sum([complexities[k] for k in key])\n",
        "        for key,_ in lang_dict.items()\n",
        "    }\n",
        "\n",
        "    return complexity"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykone2nTdzRt"
      },
      "source": [
        "# Applies to a single formula (NEW!)\n",
        "def calculate_formula_complexity(formula):\n",
        "    # a formula with nesting limit should be replaced whenever possible\n",
        "    if '((' in formula:\n",
        "        return np.inf\n",
        "    # else calculate the sum of operator complexities\n",
        "    split_formula = re.split('\\(|\\)|,', formula)\n",
        "    return sum([split_formula.count(key) * complexities[key]\n",
        "                for key in complexities.keys()])"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvlMUTLgd_02"
      },
      "source": [
        "### Languages File"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmNaJMFUuLr3"
      },
      "source": [
        "def save_languages(langs_formatted, path='/content/gdrive/My Drive/Colab Notebooks/functionally_complete'):\n",
        "    with open(f'{path}.txt', 'w', newline='\\n') as openfile:\n",
        "        openfile.write(langs_formatted)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gL5IbAJtuNRw"
      },
      "source": [
        "def update_languages_file(\n",
        "        file_path='/content/gdrive/My Drive/Colab Notebooks/functionally_complete',\n",
        "        folder_path='/content/gdrive/My Drive/Colab Notebooks/minimal_formulas',\n",
        "        verbose=True):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    file_path: str\n",
        "        Path to the file containing the list of functionally complete languages\n",
        "        which should be updated\n",
        "    folder_path: str\n",
        "        Path to the folder containing the pickle files with the minimal formulas\n",
        "    \"\"\"\n",
        "    with open(f'{file_path}.txt', 'r') as openfile:\n",
        "        currently_stored = [s.strip() for s in openfile]\n",
        "    remaining_langs = [\n",
        "        lang for lang in currently_stored\n",
        "        if not os.path.isfile(f'{folder_path}/{lang}.pickle')\n",
        "    ]\n",
        "    if verbose: print(remaining_langs)\n",
        "    langs_formatted = '\\n'.join(remaining_langs)\n",
        "    save_languages(langs_formatted, file_path)\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxPIwbxTzxyq"
      },
      "source": [
        "## Flexible Nesting Limit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZFXXvaezaLK"
      },
      "source": [
        "def flexible_nesting_limit(language_size):\n",
        "    if language_size == 1:\n",
        "        return 14\n",
        "    elif language_size == 2:\n",
        "        return 11\n",
        "    else:\n",
        "        return 8"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HiJu9XR3g_q"
      },
      "source": [
        "## Main Function\n",
        "\n",
        "Run the cells below to run the simulation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbVdaPdkhDzs"
      },
      "source": [
        "total_dict = {\n",
        "    'p': 0b1100,\n",
        "    'q': 0b1010,\n",
        "    'F': 0b0000,\n",
        "    'T': 0b1111,\n",
        "    'not': lambda x: bit_not(x),\n",
        "    'or': lambda x, y: x|y,\n",
        "    'and': lambda x, y: x&y,\n",
        "    'nand': lambda x, y: bit_not(x&y),\n",
        "    'nor': lambda x, y: bit_not(x|y),\n",
        "    'c': lambda x, y: bit_not(x)|y,\n",
        "    'ic': lambda x, y: bit_not(y)|x,\n",
        "    'nc': lambda x, y: bit_not(bit_not(x)|y),\n",
        "    'nic': lambda x, y: bit_not(bit_not(y)|x),\n",
        "    'bc': lambda x, y: (x&y)|(bit_not(x)&bit_not(y)),\n",
        "    'XOR': lambda x, y: bit_not((x&y)|(bit_not(x)&bit_not(y)))\n",
        "}"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9Vj0VRYuPWI"
      },
      "source": [
        "def update_complexity(action, primitives, verbose=True):\n",
        "\n",
        "    if action == 'all_minimal':\n",
        "        languages = get_minimal_languages('/content/gdrive/My Drive/Colab Notebooks/minimal_formulas')\n",
        "        dict_usage_complexity = calculate_expected_complexity(languages)\n",
        "        dict_cognitive_complexity = calculate_language_complexity(languages)\n",
        "        dict_complete = calculate_for_all_functionally_complete(total_dict)\n",
        "        dict_complexities_3 = calculate_expected_complexity(dict_complete)\n",
        "        if verbose:\n",
        "            pprint(list(sorted(dict_complexities_3.items(), key=lambda x:x[1])))\n",
        "        dict_complexities_1 = calculate_language_complexity(dict_complete)\n",
        "        return dict_complexities_1\n",
        "\n",
        "    elif action == 'single_minimal':\n",
        "        file_path = f'/content/gdrive/My Drive/Colab Notebooks/minimal_formulas/{primitives}.pickle'\n",
        "        primitives = primitives.split('_')\n",
        "        m_dict_restricted = {k:total_dict[k] for k in primitives + ['p', 'q']}\n",
        "        shortest = find_shortest_formulas(m_dict_restricted, verbose=verbose)\n",
        "        with open(file_path, 'wb') as openfile:\n",
        "            pickle.dump(shortest, openfile)\n",
        "        return shortest\n",
        "\n",
        "    elif action == 'single_shortest':\n",
        "        file_path = f'/content/gdrive/My Drive/Colab Notebooks/minimal_formulas/{primitives}.pickle'\n",
        "        primitives = primitives.split('_')\n",
        "        m_dict_restricted = {k:total_dict[k] for k in primitives + ['p', 'q']}\n",
        "        if verbose: print('Language: ', primitives, '\\nNesting limit: ', limit)\n",
        "        return find_shortest_formulas(m_dict_restricted, verbose = verbose)\n",
        "        \n",
        "    elif action == 'single_simplest':\n",
        "        file_path = f'/content/gdrive/My Drive/Colab Notebooks/minimal_formulas/{primitives}.pickle'\n",
        "        primitives = primitives.split('_')\n",
        "        m_dict_restricted = {k:total_dict[k] for k in primitives + ['p', 'q']}\n",
        "        limit = flexible_nesting_limit(len(primitives))\n",
        "        print('Language: ', primitives, '\\nNesting limit: ', limit)\n",
        "        simplest = find_simplest_formulas(m_dict_restricted,\n",
        "                                      nesting_limit = limit,\n",
        "                                      verbose = verbose)\n",
        "        return dict(zip(simplest[0], simplest[1]))\n",
        "\n",
        "    elif action == 'save_functionally_complete':\n",
        "        complete_langs = calculate_functionally_complete()\n",
        "        langs_formatted = '\\n'.join([\n",
        "            '_'.join(a) for a in complete_langs\n",
        "        ])\n",
        "        save_languages(langs_formatted)\n",
        "\n",
        "    elif action == 'update_languages_file':\n",
        "        update_languages_file(verbose=verbose)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eHMPSf4s2NS"
      },
      "source": [
        "### Evaluation\n",
        "This function, `update_complexity_timed`, tests how long the algorithm takes to run. These times are then plotted and averaged.\n",
        "\n",
        "**Arguments:**\n",
        "\n",
        "* `rounds`: Number of times to run the algorithm, to average over.\n",
        "* `action`: Type of action to undertake.\n",
        "  * `'all_minimal'`: Calculate the minimal formulas for all languages.\n",
        "  * `'single_minimal'`: Calculate the minimal formulas for one (given) language.\n",
        "  * `'save_functionally_complete'`: Save all functionally complete languages.\n",
        "  * `'update_languages_file'`: Update the languages file.\n",
        "* `primitives`: The primitives of a language, if `action` = `'single_minimal'`.\n",
        "* `verbose`: Whether or not to include print statements during the process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wk24AX1vjT55"
      },
      "source": [
        "def update_complexity_timed(rounds, action, primitives, verbose):\n",
        "    \n",
        "    if verbose: print('Starting...')\n",
        "    times = []\n",
        "    for i in range(rounds):\n",
        "        start = time.time()\n",
        "        update_complexity(action, primitives, verbose)\n",
        "        end = time.time()\n",
        "        if verbose: print('Round {} / {} complete'.format(i + 1, rounds))\n",
        "        times.append(end - start)\n",
        "\n",
        "    plt.plot(times)\n",
        "    plt.xlabel('Round')\n",
        "    plt.ylabel('Time (s)')\n",
        "    plt.show()\n",
        "\n",
        "    return np.mean(times)\n",
        "\n",
        "\n",
        "#print('The simulation takes an average of {} seconds.'.format(\n",
        "#    update_complexity_timed(3, 'single_minimal', 'or_bc_XOR', False)\n",
        "#    )\n",
        "#)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCpBcDRh3jy0"
      },
      "source": [
        "# Communicative Efficiency Model\n",
        "\n",
        "Based on the paper *NAND and the communicative efficiency model* by W. Uegaki (2020)\n",
        "\n",
        "Original code from: https://github.com/wuegaki/connectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3ZtPFUF8VwX"
      },
      "source": [
        "## From ConnectiveCompInfo.py\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vQxyQWx3-Kx"
      },
      "source": [
        "# Define the powerset function that generates all the subsets of a set. \n",
        "def powerset(seq):\n",
        "    if len(seq) <= 1:\n",
        "        yield seq\n",
        "        yield []\n",
        "    else:\n",
        "        for item in powerset(seq[1:]):\n",
        "            yield [seq[0]]+item\n",
        "            yield item\n",
        "\n",
        "# Optimized subset function:\n",
        "def subset(sub,sup):\n",
        "\t  return all(x in sup for x in sub)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQU6xGbOErT-"
      },
      "source": [
        "## Scalar Implicature"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etSDXH1uE_x2"
      },
      "source": [
        "### Innocently Excludable Alternatives"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ltsai6Fs-B1r"
      },
      "source": [
        "# A function returning the set of innocently excludable alternatives given a prejacent word and the language (set of words)\n",
        "def ie_fun(prej,lang):\n",
        "    # first we take the alts that are non-weaker \n",
        "    alts = []\n",
        "    for alt in lang:\n",
        "        zipobj = zip(prej,alt)\n",
        "        print([(i, j) for i, j in zipobj])\n",
        "        if any(prej_i > alt_i for prej_i, alt_i in zipobj): # If the alt is non-weaker than the prej, \n",
        "            print(alt)\n",
        "            alts = alts + [alt] # add the alt to the set of alts\n",
        "    if alts == []:\n",
        "        return []\n",
        "\n",
        "\t  # From the given set of alternatives, we derive all the possible subsets (powerset) minus the empty set. \n",
        "    power = list(powerset(alts))\n",
        "    power.remove([])\n",
        "\n",
        "    ieset = []\n",
        "    for altset in power:\n",
        "        negaltsets = map(lambda x: map(lambda y: int(not(y)),x),altset)  # This inverts the truth values of words in altset\n",
        "\t      # print altset, negaltsets  \n",
        "        withprej = negaltsets + [prej]   # combines the negated altset with the prejacent\n",
        "\t      # print withprej\n",
        "        if any(all(w[i] == 1 for w in withprej) for i in range(4)): # if withprej is consistent (i.e., if it contains any 1 across all words)\n",
        "            if ieset == []:               # in the beginning, just add the altset currently considering to the ieset\n",
        "                ieset = [altset]\n",
        "            else:                         # from the second time and on, we have to consider the maximality of the altset\n",
        "                for s in ieset:\n",
        "                    if subset(s,altset):   # if a member of ieset is already a subset of altset currently considering\n",
        "                        i = ieset.index(s)  # retrieve the index of the subset already in ieset\n",
        "                        del ieset[i]        # remove the subset from ieset\n",
        "                        ieset = ieset + [altset] # update the ieset with the altset currently considering\n",
        "                    elif not(any(subset(altset,s) for s in ieset)): # Otherwise, and if there is no member of ieset that is a superset of altset under consideration\n",
        "                        ieset = ieset + [altset] # update the ieset with the altset currently considering\t\t\t\t\n",
        "    if ieset == []:\n",
        "        return []\n",
        "\n",
        "    # get the intersection of ieset\n",
        "    ie = []\n",
        "    for alt in ieset[0]:\n",
        "        if all(alt in ieset[i] for i in range(1,len(ieset))):\n",
        "            ie = ie + [alt]\n",
        "\n",
        "    return ie"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlSCJEtYEy5O"
      },
      "source": [
        "### Strengthened Meanings and Representations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSYFwOA9-cvT"
      },
      "source": [
        "# Function that returns the strengthened meaning of a prejacent word given the prejacent word and the language\n",
        "def si(prej,lang):\n",
        "    ie = ie_fun(prej,lang)\n",
        "    if ie == []:\n",
        "\t\t    return prej\n",
        "    negie = list(map(lambda x: list(map(lambda y: int(not(y)),x)),ie))\n",
        "    combined = negie + [prej]\n",
        "    strengthened = [0]*4\n",
        "    for i in range(4):\n",
        "        if all(w[i]==1 for w in combined):\n",
        "            strengthened[i] = 1\n",
        "\t\t      # else:\n",
        "\t\t      # \tstrengthened[i] = 0\n",
        "    return strengthened"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhyTj6qh-fGL"
      },
      "source": [
        "# Function that returns the strengthened representation of a language by strengthening each word in it\n",
        "def strengthen(lang):\n",
        "\t  # new_lang = []\n",
        "\t  new_lang = [[]]*len(lang)\n",
        "\t  for i in range(len(lang)):\n",
        "\t\t    si_word = si(lang[i],lang)\n",
        "\t\t    new_lang[i] = si_word\n",
        "\t  return new_lang"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Css0oncM-hAt"
      },
      "source": [
        "# Define several utility functions\n",
        "def utility(world1,world2):\n",
        "\t  if world1 == world2:\n",
        "\t\t    return 1\n",
        "\t  elif (world1 == 0 and world2 == 3) or (world1 == 3 and world2 == 0):\n",
        "\t\t    return 1/3\n",
        "\t  else:\n",
        "\t\t    return 1/2\n",
        "\n",
        "def utility2(world1,world2):\n",
        "\t  if world1 == world2:\n",
        "\t\t    return 1\n",
        "\t  else:\n",
        "\t\t    return 0\n",
        "\n",
        "def utility3(world1,world2):\n",
        "\t  if world1 == world2:\n",
        "\t\t    return 1\n",
        "\t  elif (world1 == 0 and world2 == 3) or (world1 == 3 and world2 == 0):\n",
        "\t\t    return 0\n",
        "\t  else:\n",
        "\t\t    return 1/2"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuPl4-L0-jaL"
      },
      "source": [
        "# Function that returns the informativeness value of a language\n",
        "def info(lang):\n",
        "    info = 0\n",
        "    for world in range(4):\n",
        "        truewords = list(filter(lambda x: x[world] == 1, lang))\n",
        "        for word in truewords:\n",
        "            for world2 in range(4):\n",
        "                if word[world2] == 1:\n",
        "                    info = info + 1/len(truewords)*1/sum(word)*utility3(world,world2)\n",
        "    return info"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "us4K2H2I-3Yb"
      },
      "source": [
        "## From ConnectiveCompInfoPareto.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGqyhvV2_dC7"
      },
      "source": [
        "# Function that returns the informativeness value of a language\n",
        "def info_pareto(lang):\n",
        "\t  info = 0\n",
        "\t  for world in range(4):\n",
        "\t\t    truewords = list(filter(lambda x: x[world] == 1, lang))\n",
        "\t\t    for word in truewords:\n",
        "\t\t\t      for world2 in range(4):\n",
        "\t\t\t\t        if word[world2] == 1:\n",
        "\t\t\t\t\t          info = info + (Fraction('1/4'))*(Fraction(1/len(truewords)))*(Fraction(1/sum(word)))*utility2(world,world2)\n",
        "\n",
        "\t  return float(info)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6B4WzGySANay"
      },
      "source": [
        "#connective_names={\n",
        "#\t  \"TAU\": [1,1,1,1],\n",
        "#\t  \"<-\": [1,1,0,1],\n",
        "#\t  \"P\": [1,1,0,0],\n",
        "#\t  \"->\": [1,0,1,1],\n",
        "#\t  \"Q\": [1,0,1,0],\n",
        "#\t  \"<->\": [1,0,0,1],\n",
        "#\t  \"NAND\": [0,1,1,1],\n",
        "#\t  \"XOR\": [0,1,1,0],\n",
        "#\t  \"NOTQ\": [0,1,0,1],\n",
        "#\t  \"ONLYP\": [0,1,0,0],\n",
        "#\t  \"NOTP\": [0,0,1,1],\n",
        "#\t  \"ONLYQ\": [0,0,1,0],\n",
        "#\t  \"NOR\": [0,0,0,1],\n",
        "#\t  \"CONT\": [0,0,0,0],\n",
        "#\t  \"OR\": [1,1,1,0],\n",
        "#\t  \"AND\": [1,0,0,0],\n",
        "#\t  \"NOR\": [0,0,0,1],\n",
        "#\t  \"NAND\": [0,1,1,1]\t\n",
        "#}\n",
        "\n",
        "connective_names = {\n",
        "    'p': '1100',\n",
        "    'q': '1010',\n",
        "    'T': '1111',\n",
        "    'F': '0000',\n",
        "    'and': '1000',\n",
        "    'or': '1110',\n",
        "    'XOR': '0110',\n",
        "    'c': '1011',\n",
        "    'ic': '1101',\n",
        "    'bc': '1001',\n",
        "    'nc': '0100',\n",
        "    'nic': '0010',\n",
        "    'nand': '0111',\n",
        "    'nor': '0001',\n",
        "    'not': '0000' # ???\n",
        "}"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrCSACkSAZ4N"
      },
      "source": [
        "def name(word):\n",
        "\t  for x in connective_names:\n",
        "\t\t    if word==connective_names[x]:\n",
        "\t\t\t      wordname=x\n",
        "\t  return wordname\n",
        "\n",
        "# optimized function\t\n",
        "def names(lang):\n",
        "\t  return [name(word) for word in lang]"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTT7dUDUeqrK"
      },
      "source": [
        "# check if x dominates y (MIND THE ORDER)\n",
        "def pareto_dominant(x, y):\n",
        "    return all(x >= y) and any(x > y)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClUoXDaYApJE"
      },
      "source": [
        "# NOT the same as the original!\n",
        "def identify_pareto(efficient):\n",
        "    population_size = efficient.shape[0]\n",
        "    indices = np.arange(population_size)\n",
        "    pareto_front = np.ones(population_size, dtype=bool)\n",
        "    # Loop through each item. This will then be compared with all other items\n",
        "    for i in range(population_size):\n",
        "        for j in range(population_size):\n",
        "            # Check if our 'i' point is dominated by our 'j' point\n",
        "            if pareto_dominant(efficient[j], efficient[i]):\n",
        "                pareto_front[i] = 0\n",
        "                break\n",
        "    # Return indices on Pareto front\n",
        "    return indices[pareto_front]"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZN_iU-R035HK"
      },
      "source": [
        "# Combining Update Semantics with Communicative Efficiency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZlDg6t-f9Yg"
      },
      "source": [
        "# looks up a formula in a dictionary as given by a meaning\n",
        "# and returns it and its complexity if they exist, otherwise '-' and inf\n",
        "def check_formula_complexity(formula_dict, key):\n",
        "    if key in formula_dict.keys():\n",
        "        formula = formula_dict[key]\n",
        "        if '((' in formula:\n",
        "            complexity = np.inf\n",
        "        else:\n",
        "            complexity = calculate_formula_complexity(formula)\n",
        "    else:\n",
        "        formula = '-'\n",
        "        complexity = np.inf\n",
        "    return formula, complexity"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDSjsGtwjdFJ"
      },
      "source": [
        "def expected_cognitive_complexity(formula_dict):\n",
        "    \"\"\"\n",
        "    Calculates expected complexity for ONE lang,\n",
        "    where complexity is calculated as the number of operations \n",
        "    in the formula, based on COGNITIVE COMPLEXITY.\n",
        "    Parameters\n",
        "    ----------\n",
        "    dict_complete: dict\n",
        "        Dictionary where each key is a tuple (or ordered dict) of primitives\n",
        "        and each value is \n",
        "    \"\"\"\n",
        "    return np.mean([check_formula_complexity(formula_dict, key)[1]\n",
        "                    for key in connective_str])"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6VAR89Wm-ST"
      },
      "source": [
        "## Comparing Complexity Measures"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvFw-cfmncVp"
      },
      "source": [
        "#test_language = 'nand'\n",
        "test_language = 'and_or_not_c'\n",
        "#test_language = 'and_or_not_nor_c_nc_ic_bc_nic_XOR_nand'"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYc_mRMXmx6a"
      },
      "source": [
        "shortest = update_complexity('single_minimal',\n",
        "                            test_language,\n",
        "                            verbose=False)\n",
        "formula_dict_shortest = dict(zip(shortest[0], shortest[1]))"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBNSkN1Rm05S",
        "outputId": "48743fe8-ec45-4755-86ab-0dbcd8c4d7fa"
      },
      "source": [
        "formula_dict_simplest = update_complexity('single_simplest',\n",
        "                                          test_language,\n",
        "                                          verbose=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Language:  ['and', 'or', 'not', 'c'] \n",
            "Nesting limit:  8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 558
        },
        "id": "DxP328xzW3eW",
        "outputId": "87b692c9-82a2-4d31-d663-8fce9abba51c"
      },
      "source": [
        "connective_values1_str = {\n",
        "\t  '1110': 1,\n",
        "\t  '1000': 1,\n",
        "\t  '1101': 2,\n",
        "\t  '1011': 2, \n",
        "\t  '0111': 2,\n",
        "\t  '0100': 2,\n",
        "\t  '0010': 2,\n",
        "\t  '0001': 2,\n",
        "\t  '1100': 3,\n",
        "\t  '1010': 3,\n",
        "\t  '0000': 3,\n",
        "\t  '1111': 4,\n",
        "\t  '1001': 4,\n",
        "\t  '0110': 4,\n",
        "\t  '0101': 4,\n",
        "\t  '0011': 4\n",
        "}\n",
        "\n",
        "data = []\n",
        "for key in connective_values1_str.keys():\n",
        "    f_shortest, c_shortest = check_formula_complexity(formula_dict_shortest, key)\n",
        "    f_simplest, c_simplest = check_formula_complexity(formula_dict_simplest, key)\n",
        "    data += [{'truth table':key,\n",
        "              'shortest formula': f_shortest,\n",
        "              #'basic complexity': connective_values1_str[key],\n",
        "              'length 1': f_shortest.count('('),\n",
        "              'shortest complexity': c_shortest,\n",
        "              'simplest formula': f_simplest,\n",
        "              'length 2': f_simplest.count('('),\n",
        "              'cognitive complexity': c_simplest,\n",
        "              'difference': c_shortest - c_simplest\n",
        "              }]\n",
        "\n",
        "test_data = pd.DataFrame(data)\n",
        "test_data.to_csv(f'/content/gdrive/My Drive/Colab Notebooks/{test_language}.csv')\n",
        "test_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>truth table</th>\n",
              "      <th>shortest formula</th>\n",
              "      <th>basic complexity</th>\n",
              "      <th>length 1</th>\n",
              "      <th>shortest complexity</th>\n",
              "      <th>simplest formula</th>\n",
              "      <th>length 2</th>\n",
              "      <th>cognitive complexity</th>\n",
              "      <th>difference</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1110</td>\n",
              "      <td>or(p,q)</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>or(p,q)</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1000</td>\n",
              "      <td>and(p,q)</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>and(p,q)</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1101</td>\n",
              "      <td>c(q,p)</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4.0</td>\n",
              "      <td>or(p,not(q))</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1011</td>\n",
              "      <td>c(p,q)</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4.0</td>\n",
              "      <td>or(not(p),q)</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0111</td>\n",
              "      <td>not(and(p,q))</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3.0</td>\n",
              "      <td>not(and(p,q))</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0100</td>\n",
              "      <td>and(p,not(q))</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3.0</td>\n",
              "      <td>and(p,not(q))</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0010</td>\n",
              "      <td>and(q,not(p))</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3.0</td>\n",
              "      <td>and(not(p),q)</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0001</td>\n",
              "      <td>not(or(p,q))</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3.0</td>\n",
              "      <td>not(or(p,q))</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1100</td>\n",
              "      <td>p</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>p</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1010</td>\n",
              "      <td>q</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>q</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0000</td>\n",
              "      <td>((((((((</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>inf</td>\n",
              "      <td>and(p,not(p))</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>inf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1111</td>\n",
              "      <td>c(p,p)</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>4.0</td>\n",
              "      <td>or(p,not(p))</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1001</td>\n",
              "      <td>and(c(p,q),c(q,p))</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>9.0</td>\n",
              "      <td>or(and(p,q),not(or(p,q)))</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0110</td>\n",
              "      <td>-</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>inf</td>\n",
              "      <td>and(or(p,q),not(and(p,q)))</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>inf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0101</td>\n",
              "      <td>not(q)</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>not(q)</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0011</td>\n",
              "      <td>not(p)</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>not(p)</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   truth table    shortest formula  ...  cognitive complexity  difference\n",
              "0         1110             or(p,q)  ...                     1         0.0\n",
              "1         1000            and(p,q)  ...                     1         0.0\n",
              "2         1101              c(q,p)  ...                     3         1.0\n",
              "3         1011              c(p,q)  ...                     3         1.0\n",
              "4         0111       not(and(p,q))  ...                     3         0.0\n",
              "5         0100       and(p,not(q))  ...                     3         0.0\n",
              "6         0010       and(q,not(p))  ...                     3         0.0\n",
              "7         0001        not(or(p,q))  ...                     3         0.0\n",
              "8         1100                   p  ...                     0         0.0\n",
              "9         1010                   q  ...                     0         0.0\n",
              "10        0000            ((((((((  ...                     3         inf\n",
              "11        1111              c(p,p)  ...                     3         1.0\n",
              "12        1001  and(c(p,q),c(q,p))  ...                     5         4.0\n",
              "13        0110                   -  ...                     5         inf\n",
              "14        0101              not(q)  ...                     2         0.0\n",
              "15        0011              not(p)  ...                     2         0.0\n",
              "\n",
              "[16 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEZ9JDlD_DEJ"
      },
      "source": [
        "## Computing Pareto Frontier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7Zj2I6xN5m0"
      },
      "source": [
        "connective_str = ['1111', '1110', '1101', '1100', '1011', '1010', '1001', '1000', '0111', '0110', '0101', '0100', '0011', '0010', '0001', '0000']"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_SwcqXdnbkI"
      },
      "source": [
        "# turns a truth value string into a list\n",
        "# i.e. '1010' to [1, 0, 1, 0]\n",
        "def string_to_bits(c):\n",
        "    return list([int(i) for i in c])"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Td2GmUEvYtx4"
      },
      "source": [
        "### Custom Strengthening"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6B7ygfOVCiQ"
      },
      "source": [
        "# Modified version of ie_fun\n",
        "def ie_cognitive(prej,lang):\n",
        "\n",
        "    # first we take the alts that are non-weaker\n",
        "    alts = []\n",
        "    for alt in lang:\n",
        "        # If the alt is non-weaker than the prej\n",
        "        if any(prej_i > alt_i for prej_i, alt_i in list(zip(prej,alt))):  \n",
        "            alts = alts + [alt]\n",
        "    if alts == []:\n",
        "        return []\n",
        " \n",
        "    power = list(powerset(alts))\n",
        "    power.remove([])\n",
        "\n",
        "    ieset = []\n",
        "    for altset in power:\n",
        "        # This inverts the truth values of words in altset\n",
        "        #negaltsets = map(lambda x: map(lambda y: int(not(y)),x),altset)  \n",
        "        negaltsets = list(map(lambda x: list(map(lambda y: int(not(y)),x)),altset))\n",
        "        withprej = negaltsets + [prej]   # combines the negated altset with the prejacent\n",
        "        if any(all(w[i] == 1 for w in withprej) for i in range(4)): # if withprej is consistent (i.e., if it contains any 1 across all words)\n",
        "            if ieset == []:               # in the beginning, just add the altset currently considering to the ieset\n",
        "                ieset = [altset]\n",
        "            else:                         # from the second time and on, we have to consider the maximality of the altset\n",
        "                for s in ieset:\n",
        "                    if subset(s,altset):   # if a member of ieset is already a subset of altset currently considering\n",
        "                        i = ieset.index(s)  # retrieve the index of the subset already in ieset\n",
        "                        del ieset[i]        # remove the subset from ieset\n",
        "                        ieset = ieset + [altset] # update the ieset with the altset currently considering\n",
        "                    elif not(any(subset(altset,s) for s in ieset)): # Otherwise, and if there is no member of ieset that is a superset of altset under consideration\n",
        "                        ieset = ieset + [altset] # update the ieset with the altset currently considering\t\t\t\t\n",
        "    if ieset == []:\n",
        "        return []\n",
        "\n",
        "    # get the intersection of ieset\n",
        "    ie = []\n",
        "    for alt in ieset[0]:\n",
        "        if all(alt in ieset[i] for i in range(1,len(ieset))):\n",
        "            ie = ie + [alt]\n",
        "\n",
        "    return ie"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-6iIc-hYiCP"
      },
      "source": [
        "# Function that returns the strengthened meaning of a prejacent word\n",
        "# given the prejacent word and the language\n",
        "# (modified version of si())\n",
        "def si_cognitive(prej,lang):\n",
        "    ie = ie_cognitive(prej,lang)\n",
        "    if ie == []:\n",
        "\t\t    return prej\n",
        "    negie = list(map(lambda x: list(map(lambda y: int(not(y)),x)),ie))\n",
        "    return [int(all(w[i]==1 for w in negie + [prej])) for i in range(4)]"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIKUa5Zyx7tg"
      },
      "source": [
        "## Final Simulation\n",
        "\n",
        "The code below randomly generates languages within a given range of lengths, with the option of limiting the generation to only functionally complete ones. Then, it generates a table of cognitive complexity (Carcassi and Sbardolini, 2021) and informativeness (Uegaki, 2020) and computes the Pareto frontier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kVEpiKV-UG1"
      },
      "source": [
        "# check if a language is functionally complete\n",
        "# by seeing if any of the f_complete languages is a subset of lang\n",
        "def check_f_complete(lang):\n",
        "    return any(subset(f, lang) for f in f_complete)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeHhOZ3YEZua"
      },
      "source": [
        "# generate a set of languages\n",
        "def generate_languages(min_length=0, max_length=5, complete_only=False):\n",
        "    languages = list(powerset(list(total_dict.keys())[2:])) # remove p and q!\n",
        "    languages.remove([])\n",
        "    if complete_only:\n",
        "        languages = [lang for lang in languages if check_f_complete(lang)]\n",
        "    return [lang for lang in languages\n",
        "            if len(lang) in range(min_length, max_length + 1)]"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQdFk98HyTqc"
      },
      "source": [
        "# a version of the main function without joining things together with _\n",
        "def quick_simplest(primitives, verbose):\n",
        "    m_dict_restricted = {k:total_dict[k] for k in primitives + ['p', 'q']}\n",
        "    limit = flexible_nesting_limit(len(primitives))\n",
        "    print('Language: ', primitives, '\\nNesting limit: ', limit)\n",
        "    simplest = find_simplest_formulas(m_dict_restricted,\n",
        "                                      nesting_limit = limit,\n",
        "                                      verbose = verbose)\n",
        "    print('Done. Truth values found:', len(simplest[0]))\n",
        "    return dict(zip(simplest[0], simplest[1]))"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKzurgkKJ0Es"
      },
      "source": [
        "# Modified version of pareto code to incorporate cognitive complexity\n",
        "# Returns a pandas dataframe to use as you please\n",
        "def pareto_table(min_length=0, max_length=6, verbose=False,\n",
        "                     complete_only=False):\n",
        "    # you can choose to only use functionally complete languages\n",
        "    langs = generate_languages(min_length, max_length, complete_only)\n",
        "\n",
        "    # Only the f_complete languages;\n",
        "    # remove/comment this line to execute properly\n",
        "    langs = [list(f) for f in f_complete]\n",
        "\n",
        "    langs = sorted(langs, key=len) # Sort by length (number of primitives)\n",
        "    #np.random.shuffle(langs)\n",
        "    print('Languages:', len(langs))\n",
        "    table = []\n",
        "    for lang in langs:\n",
        "        # find simplest formulas\n",
        "        f_simplest = quick_simplest(lang, verbose)\n",
        "        # strengthened meanings (truth table values as defined by Uegaki)\n",
        "        meanings = [string_to_bits(connective_names[word]) for word in lang]\n",
        "        meanings = [si_cognitive(m, meanings) for m in meanings]\n",
        "        table += [{'Language': '_'.join(lang),\n",
        "                      'Complexity': expected_cognitive_complexity(f_simplest),\n",
        "                      'Informativeness': info(meanings)\n",
        "                      }]\n",
        "    \n",
        "    return pd.DataFrame(table)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpFJh_hCSyaY",
        "outputId": "efa5e0c7-031c-47c1-ab0d-9c1aa30ed350"
      },
      "source": [
        "table = pareto_table(0, 5, False, True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Languages: 18\n",
            "Language:  ['nor'] \n",
            "Nesting limit:  14\n",
            "Done. Truth values found: 16\n",
            "Language:  ['nand'] \n",
            "Nesting limit:  14\n",
            "Done. Truth values found: 16\n",
            "Language:  ['or', 'not'] \n",
            "Nesting limit:  11\n",
            "Done. Truth values found: 16\n",
            "Language:  ['and', 'not'] \n",
            "Nesting limit:  11\n",
            "Done. Truth values found: 16\n",
            "Language:  ['c', 'not'] \n",
            "Nesting limit:  11\n",
            "Done. Truth values found: 16\n",
            "Language:  ['ic', 'not'] \n",
            "Nesting limit:  11\n",
            "Done. Truth values found: 16\n",
            "Language:  ['c', 'XOR'] \n",
            "Nesting limit:  11\n",
            "Done. Truth values found: 16\n",
            "Language:  ['ic', 'XOR'] \n",
            "Nesting limit:  11\n",
            "Done. Truth values found: 16\n",
            "Language:  ['c', 'nc'] \n",
            "Nesting limit:  11\n",
            "Done. Truth values found: 16\n",
            "Language:  ['c', 'nic'] \n",
            "Nesting limit:  11\n",
            "Done. Truth values found: 16\n",
            "Language:  ['ic', 'nc'] \n",
            "Nesting limit:  11\n",
            "Done. Truth values found: 16\n",
            "Language:  ['ic', 'nic'] \n",
            "Nesting limit:  11\n",
            "Done. Truth values found: 14\n",
            "Language:  ['nc', 'not'] \n",
            "Nesting limit:  11\n",
            "Done. Truth values found: 16\n",
            "Language:  ['nic', 'not'] \n",
            "Nesting limit:  11\n",
            "Done. Truth values found: 16\n",
            "Language:  ['nc', 'bc'] \n",
            "Nesting limit:  11\n",
            "Done. Truth values found: 16\n",
            "Language:  ['nic', 'bc'] \n",
            "Nesting limit:  11\n",
            "Done. Truth values found: 16\n",
            "Language:  ['or', 'bc', 'XOR'] \n",
            "Nesting limit:  8\n",
            "Done. Truth values found: 16\n",
            "Language:  ['and', 'bc', 'XOR'] \n",
            "Nesting limit:  8\n",
            "Done. Truth values found: 16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAPwIB1NDnJj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 603
        },
        "outputId": "9b6bd01d-fccb-41a7-fbf1-735561024027"
      },
      "source": [
        "# Save the data to a csv for later\n",
        "table.to_csv('/content/gdrive/My Drive/Colab Notebooks/table.csv', index=False)\n",
        "table"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Language</th>\n",
              "      <th>Complexity</th>\n",
              "      <th>Informativeness</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>nor</td>\n",
              "      <td>6.000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>nand</td>\n",
              "      <td>12.000</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>or_not</td>\n",
              "      <td>4.125</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>and_not</td>\n",
              "      <td>4.125</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>c_not</td>\n",
              "      <td>6.250</td>\n",
              "      <td>1.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>ic_not</td>\n",
              "      <td>6.750</td>\n",
              "      <td>1.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>c_XOR</td>\n",
              "      <td>7.500</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>ic_XOR</td>\n",
              "      <td>7.500</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>c_nc</td>\n",
              "      <td>6.125</td>\n",
              "      <td>2.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>c_nic</td>\n",
              "      <td>6.875</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>ic_nc</td>\n",
              "      <td>6.875</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>ic_nic</td>\n",
              "      <td>inf</td>\n",
              "      <td>2.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>nc_not</td>\n",
              "      <td>5.750</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>nic_not</td>\n",
              "      <td>5.250</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>nc_bc</td>\n",
              "      <td>6.500</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>nic_bc</td>\n",
              "      <td>6.375</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>or_bc_XOR</td>\n",
              "      <td>5.000</td>\n",
              "      <td>3.083333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>and_bc_XOR</td>\n",
              "      <td>5.000</td>\n",
              "      <td>3.500000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Language  Complexity  Informativeness\n",
              "0          nor       6.000         1.000000\n",
              "1         nand      12.000         2.000000\n",
              "2       or_not       4.125         2.000000\n",
              "3      and_not       4.125         1.000000\n",
              "4        c_not       6.250         1.666667\n",
              "5       ic_not       6.750         1.666667\n",
              "6        c_XOR       7.500         2.000000\n",
              "7       ic_XOR       7.500         2.000000\n",
              "8         c_nc       6.125         2.666667\n",
              "9        c_nic       6.875         2.000000\n",
              "10       ic_nc       6.875         2.000000\n",
              "11      ic_nic         inf         2.666667\n",
              "12      nc_not       5.750         1.000000\n",
              "13     nic_not       5.250         1.000000\n",
              "14       nc_bc       6.500         2.000000\n",
              "15      nic_bc       6.375         2.000000\n",
              "16   or_bc_XOR       5.000         3.083333\n",
              "17  and_bc_XOR       5.000         3.500000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbsMf8j_nRoG"
      },
      "source": [
        "### Plotting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0i5RQRkvK9MN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "outputId": "7cc2ccb8-b021-4c1f-8d82-06f1c8fd57e9"
      },
      "source": [
        "# Use the data to compute the Pareto frontier\n",
        "table_np = np.array(pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/table.csv',\n",
        "                    usecols=['Language', 'Complexity', 'Informativeness']))\n",
        "# we want to minimize complexity, not maximize it\n",
        "pareto = identify_pareto(np.array([[-i[1], i[2]] for i in table_np]))\n",
        "pareto_front = table_np[pareto]\n",
        "\n",
        "attested = np.array([row for row in table_np\n",
        "                     if any(all(i in row[0].split('_') for i in a)\n",
        "                     for a in attested_languages)])\n",
        "\n",
        "plt.scatter(table_np[:,1], table_np[:,2], label='All')\n",
        "plt.scatter(pareto_front[:,1], pareto_front[:,2], s=64, label='Pareto-optimal')\n",
        "plt.scatter(attested[:,1], attested[:,2], facecolors='none', edgecolors='g',\n",
        "            s=100, label='Attested')\n",
        "# annotate special languages\n",
        "langs_to_mark = np.vstack({tuple(lang)\n",
        "                           for lang in np.concatenate((attested, pareto_front),\n",
        "                                                      axis=0)})\n",
        "for lang, x, y in langs_to_mark:\n",
        "    plt.annotate(lang.split('_'), (float(x), float(y)),\n",
        "                 textcoords='offset points',\n",
        "                 xytext=(0, 10), ha='center')\n",
        "plt.margins(0.15)\n",
        "plt.xlabel('Complexity')\n",
        "plt.ylabel('Informativeness')\n",
        "plt.legend(title='Languages')\n",
        "plt.savefig('/content/gdrive/My Drive/Colab Notebooks/pareto.png', dpi=200)\n",
        "plt.show()\n",
        "\n",
        "print(\"Connectives:\", list(total_dict.keys()))\n",
        "print(\"Pareto frontier:\", pareto_front)"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Connectives: ['p', 'q', 'F', 'T', 'not', 'or', 'and', 'nand', 'nor', 'c', 'ic', 'nc', 'nic', 'bc', 'XOR']\n",
            "Pareto frontier: [['or_not' 4.125 2.0]\n",
            " ['and_bc_XOR' 5.0 3.5]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}